# Airflow Plugin - IBM Watson Campaign Automation 

This plugin moves data from the [IBM Watson Campaign Automation XML API](https://developer.ibm.com/customer-engagement/docs/watson-marketing/ibm-engage-2/watson-campaign-automation-platform/xml-api/api-xml-overview/) to Google Cloud Storage based on the specified List Id.

## Dependencies

This plugin requires the following packages

```
lxml
xmljson
```

## Connection

Create a new HTTP Connection with the following parameters

* Connection ID naming convention: `ibm_wca_{org nickname}` i.e. `ibm_wca_fdr`.
* __Host__: `api{your Pod number}.ibmmarketingcloud.com` i.e. `api3.ibmmarketingcloud.com`.
* __Password__: `refresh_token` generated by IBM WCA.
* __Extra__: JSON string containing the following properties
    - `pod`: IBM WCA Pod number
    - `client_id`: IBM WCA client_id
    - `client_secret`: IBM WCA client_secret

    Example: `{ "pod": 3, "client_id": "xxxxxxxxxxxxxxxxx", "client_secret": "xxxxxxxxxxxxxxxxx" }`

## Hooks

### IBM Watson Campaign Automation Hook

Handles the authentication and requests to the XML API as well as FTP server by extending the [HttpHook](https://airflow.apache.org/code.html?highlight=http_hook#airflow.hooks.http_hook.HttpHook) and [FTPHook](https://airflow.apache.org/code.html?highlight=ftp_hook#airflow.contrib.hooks.ftp_hook.FTPHook). This hook allows you to make OAuth authenticated calls to import and export data via XML API and securely interact with FTP server to move the target files to and from Google Cloud Storage.

### Google Cloud Storage Hook

[Contrib Airflow GoogleCloudStorageHook](https://airflow.readthedocs.io/en/stable/_modules/airflow/contrib/hooks/gcs_hook.html) interacts with Google Cloud Storage. This hook uses the Google Cloud Platform connection.

## Operators

### IbmWcaExportListToGCSOperator

Calls the [`ExportList`](https://developer.ibm.com/customer-engagement/tutorials/export-from-a-database/) XML API endpoint to initiate a list export, polls the [`GetJobStatus`](https://developer.ibm.com/customer-engagement/tutorials/get-status-data-job/) endpoint until the job is complete and transfers the output file from FTP server to Google Cloud Storage.

Accepts the following parameters:

    :param ibm_wca_conn_id:   IBM WCA Connection Id (HTTP Connection)
    :param list_id:           IBM WCA List Id to export.
    :param gcs_conn_id:       GCS Connection Id
    :param gcs_bucket:        GCS bucket name
    :param gcs_key:           GCS destination filename

## Example DAG

```python
import datetime

from airflow import models
from airflow import utils
from airflow.operators import IbmWcaExportListToGCSOperator

default_dag_args = {"start_date": utils.dates.days_ago(2)}

with models.DAG(
    "ibm_export_list_to_gcs",
    schedule_interval=datetime.timedelta(days=1),
    default_args=default_dag_args,
) as dag:

    export_list_to_gcs = IbmWcaExportListToGCSOperator(
        task_id="export_list_to_gcs",
        ibm_wca_conn_id="ibm_wca_fdr_sb",
        list_id=123456,
        gcs_bucket="simo_123_composer_staging",
        gcs_key="database_123456.csv",
    )
    export_list_to_gcs
```